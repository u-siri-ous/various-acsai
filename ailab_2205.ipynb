{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9VjrQnMvozu4EDCFhzJ4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/u-siri-ous/various-acsai/blob/ai-lab/ailab_2205.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wmDEDuc5rBR0",
        "outputId": "0b9b27a8-605d-492f-9045-0dafcba976cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "loss: 2.1965157985687256 [1/60000]\n",
            "loss: 2.4059743881225586 [501/60000]\n",
            "loss: 2.1076297760009766 [1001/60000]\n",
            "loss: 2.1210134029388428 [1501/60000]\n",
            "loss: 2.471747875213623 [2001/60000]\n",
            "loss: 2.368439197540283 [2501/60000]\n",
            "loss: 2.4624931812286377 [3001/60000]\n",
            "loss: 2.3818600177764893 [3501/60000]\n",
            "loss: 2.64270281791687 [4001/60000]\n",
            "loss: 2.098660707473755 [4501/60000]\n",
            "loss: 2.413151264190674 [5001/60000]\n",
            "loss: 2.368678331375122 [5501/60000]\n",
            "loss: 2.561051845550537 [6001/60000]\n",
            "loss: 2.35410475730896 [6501/60000]\n",
            "loss: 2.231210470199585 [7001/60000]\n",
            "loss: 2.245044231414795 [7501/60000]\n",
            "loss: 2.2836270332336426 [8001/60000]\n",
            "loss: 2.2076797485351562 [8501/60000]\n",
            "loss: 2.3202826976776123 [9001/60000]\n",
            "loss: 2.297813653945923 [9501/60000]\n",
            "loss: 2.4631850719451904 [10001/60000]\n",
            "loss: 2.2978556156158447 [10501/60000]\n",
            "loss: 2.4415526390075684 [11001/60000]\n",
            "loss: 2.3514389991760254 [11501/60000]\n",
            "loss: 2.3011908531188965 [12001/60000]\n",
            "loss: 2.426544666290283 [12501/60000]\n",
            "loss: 2.25901460647583 [13001/60000]\n",
            "loss: 2.231524705886841 [13501/60000]\n",
            "loss: 2.294778823852539 [14001/60000]\n",
            "loss: 2.3917977809906006 [14501/60000]\n",
            "loss: 2.2727367877960205 [15001/60000]\n",
            "loss: 2.3877735137939453 [15501/60000]\n",
            "loss: 2.2964582443237305 [16001/60000]\n",
            "loss: 2.2940680980682373 [16501/60000]\n",
            "loss: 2.306478261947632 [17001/60000]\n",
            "loss: 2.273911714553833 [17501/60000]\n",
            "loss: 2.386209011077881 [18001/60000]\n",
            "loss: 2.2975215911865234 [18501/60000]\n",
            "loss: 2.299652338027954 [19001/60000]\n",
            "loss: 2.3228919506073 [19501/60000]\n",
            "loss: 2.3074827194213867 [20001/60000]\n",
            "loss: 2.2971878051757812 [20501/60000]\n",
            "loss: 2.3125343322753906 [21001/60000]\n",
            "loss: 2.365232467651367 [21501/60000]\n",
            "loss: 2.297592878341675 [22001/60000]\n",
            "loss: 2.290754556655884 [22501/60000]\n",
            "loss: 2.3112375736236572 [23001/60000]\n",
            "loss: 2.3024373054504395 [23501/60000]\n",
            "loss: 2.3153438568115234 [24001/60000]\n",
            "loss: 2.2836825847625732 [24501/60000]\n",
            "loss: 2.283294439315796 [25001/60000]\n",
            "loss: 2.260444402694702 [25501/60000]\n",
            "loss: 2.303337812423706 [26001/60000]\n",
            "loss: 2.308936595916748 [26501/60000]\n",
            "loss: 2.3025193214416504 [27001/60000]\n",
            "loss: 2.2956109046936035 [27501/60000]\n",
            "loss: 2.3143246173858643 [28001/60000]\n",
            "loss: 2.318796396255493 [28501/60000]\n",
            "loss: 2.283853530883789 [29001/60000]\n",
            "loss: 2.3000338077545166 [29501/60000]\n",
            "loss: 2.2987477779388428 [30001/60000]\n",
            "loss: 2.31001615524292 [30501/60000]\n",
            "loss: 2.2953217029571533 [31001/60000]\n",
            "loss: 2.312610387802124 [31501/60000]\n",
            "loss: 2.3135650157928467 [32001/60000]\n",
            "loss: 2.2917680740356445 [32501/60000]\n",
            "loss: 2.290487051010132 [33001/60000]\n",
            "loss: 2.2734649181365967 [33501/60000]\n",
            "loss: 2.2859113216400146 [34001/60000]\n",
            "loss: 2.3084864616394043 [34501/60000]\n",
            "loss: 2.2929365634918213 [35001/60000]\n",
            "loss: 2.281521797180176 [35501/60000]\n",
            "loss: 2.319943428039551 [36001/60000]\n",
            "loss: 2.2585670948028564 [36501/60000]\n",
            "loss: 2.3042454719543457 [37001/60000]\n",
            "loss: 2.3040170669555664 [37501/60000]\n",
            "loss: 2.2802484035491943 [38001/60000]\n",
            "loss: 2.285276412963867 [38501/60000]\n",
            "loss: 2.333993434906006 [39001/60000]\n",
            "loss: 2.3348371982574463 [39501/60000]\n",
            "loss: 2.2644472122192383 [40001/60000]\n",
            "loss: 2.2669808864593506 [40501/60000]\n",
            "loss: 2.337207555770874 [41001/60000]\n",
            "loss: 2.294029712677002 [41501/60000]\n",
            "loss: 2.28879451751709 [42001/60000]\n",
            "loss: 2.278672456741333 [42501/60000]\n",
            "loss: 2.3178086280822754 [43001/60000]\n",
            "loss: 2.298722982406616 [43501/60000]\n",
            "loss: 2.3217861652374268 [44001/60000]\n",
            "loss: 2.2777280807495117 [44501/60000]\n",
            "loss: 2.3139569759368896 [45001/60000]\n",
            "loss: 2.295468330383301 [45501/60000]\n",
            "loss: 2.2770423889160156 [46001/60000]\n",
            "loss: 2.3148181438446045 [46501/60000]\n",
            "loss: 2.2875163555145264 [47001/60000]\n",
            "loss: 2.309406280517578 [47501/60000]\n",
            "loss: 2.3096418380737305 [48001/60000]\n",
            "loss: 2.3083863258361816 [48501/60000]\n",
            "loss: 2.288693904876709 [49001/60000]\n",
            "loss: 2.2566936016082764 [49501/60000]\n",
            "loss: 2.306248188018799 [50001/60000]\n",
            "loss: 2.2728915214538574 [50501/60000]\n",
            "loss: 2.295593023300171 [51001/60000]\n",
            "loss: 2.318570613861084 [51501/60000]\n",
            "loss: 2.2971928119659424 [52001/60000]\n",
            "loss: 2.2673919200897217 [52501/60000]\n",
            "loss: 2.298109292984009 [53001/60000]\n",
            "loss: 2.2657124996185303 [53501/60000]\n",
            "loss: 2.2797374725341797 [54001/60000]\n",
            "loss: 2.3088252544403076 [54501/60000]\n",
            "loss: 2.292863130569458 [55001/60000]\n",
            "loss: 2.291273355484009 [55501/60000]\n",
            "loss: 2.306849479675293 [56001/60000]\n",
            "loss: 2.3289475440979004 [56501/60000]\n",
            "loss: 2.297090768814087 [57001/60000]\n",
            "loss: 2.297694683074951 [57501/60000]\n",
            "loss: 2.2925288677215576 [58001/60000]\n",
            "loss: 2.316326141357422 [58501/60000]\n",
            "loss: 2.3178558349609375 [59001/60000]\n",
            "loss: 2.3031792640686035 [59501/60000]\n",
            "epoch: 1\n",
            "loss: 2.2729878425598145 [1/60000]\n",
            "loss: 2.301950454711914 [501/60000]\n",
            "loss: 2.2996015548706055 [1001/60000]\n",
            "loss: 2.2992794513702393 [1501/60000]\n",
            "loss: 2.270124673843384 [2001/60000]\n",
            "loss: 2.294539451599121 [2501/60000]\n",
            "loss: 2.3291425704956055 [3001/60000]\n",
            "loss: 2.2984201908111572 [3501/60000]\n",
            "loss: 2.310697555541992 [4001/60000]\n",
            "loss: 2.284787654876709 [4501/60000]\n",
            "loss: 2.2782115936279297 [5001/60000]\n",
            "loss: 2.3033764362335205 [5501/60000]\n",
            "loss: 2.3044075965881348 [6001/60000]\n",
            "loss: 2.294583559036255 [6501/60000]\n",
            "loss: 2.26638126373291 [7001/60000]\n",
            "loss: 2.283015012741089 [7501/60000]\n",
            "loss: 2.2954652309417725 [8001/60000]\n",
            "loss: 2.2747538089752197 [8501/60000]\n",
            "loss: 2.294588088989258 [9001/60000]\n",
            "loss: 2.3061466217041016 [9501/60000]\n",
            "loss: 2.3030502796173096 [10001/60000]\n",
            "loss: 2.2746922969818115 [10501/60000]\n",
            "loss: 2.295668363571167 [11001/60000]\n",
            "loss: 2.2911736965179443 [11501/60000]\n",
            "loss: 2.2606801986694336 [12001/60000]\n",
            "loss: 2.3022005558013916 [12501/60000]\n",
            "loss: 2.274104118347168 [13001/60000]\n",
            "loss: 2.260958433151245 [13501/60000]\n",
            "loss: 2.335205316543579 [14001/60000]\n",
            "loss: 2.297557830810547 [14501/60000]\n",
            "loss: 2.318958044052124 [15001/60000]\n",
            "loss: 2.3035192489624023 [15501/60000]\n",
            "loss: 2.297179937362671 [16001/60000]\n",
            "loss: 2.281406879425049 [16501/60000]\n",
            "loss: 2.304370880126953 [17001/60000]\n",
            "loss: 2.281158685684204 [17501/60000]\n",
            "loss: 2.3240036964416504 [18001/60000]\n",
            "loss: 2.3032143115997314 [18501/60000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fbc039905e27>\u001b[0m in \u001b[0;36m<cell line: 124>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch: {t}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'daje lupetti'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-fbc039905e27>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# backward pass (backpropagation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;31m# backpropagate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;31m# compute derivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# clean gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# print(torch.cuda.is_available()) # cannot use GPU acceleration, daje\n",
        "\n",
        "# create a tensor from a list\n",
        "data = [\n",
        "    [1,2],\n",
        "    [3,4]\n",
        "]\n",
        "\n",
        "pyt_data = torch.tensor(data) #or\n",
        "#pyt_data = torch.from_numpy(data)\n",
        "\n",
        "# import train and testing data\n",
        "training_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=ToTensor()) \n",
        "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=ToTensor()) \n",
        "\n",
        "# labels of the dataset (also found online)\n",
        "labels_map = {\n",
        "    0: 'tshirt',\n",
        "    1: 'trousers',\n",
        "    2: 'pullover',\n",
        "    3: 'dress',\n",
        "    4: 'coat',\n",
        "    5: 'sandal',\n",
        "    6: 'shirt',\n",
        "    7: 'sneaker',\n",
        "    8: 'bag',\n",
        "    9: 'ankle boot'\n",
        "}\n",
        "\n",
        "\"\"\" # visualize the model\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols*rows + 1):\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
        "    img, label = training_data[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)   # filling cells of the matrix\n",
        "    plt.title(labels_map[label])    # putting its label as title\n",
        "    plt.axis('off')\n",
        "    plt.imshow(img.squeeze(), cmap='gray')      # squeeze() deletes cardinality 1 dimensions \n",
        "\n",
        "plt.show()\n",
        " \"\"\"\n",
        "\n",
        "# create the model / neural network\n",
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# create a model class, which inherits from nn\n",
        "class OurMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # method 1 - using Sequential\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(28*28, 50),  # input layer, we can specify input and output size\n",
        "            nn.Sigmoid(),          # we can specify different activation function in between different hidden layers\n",
        "            nn.Linear(50, 50),     # pay attention, as output of previous hidden layers should match the input of the next one\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(50, 10)      # the last output should match the number of classes of the model\n",
        "        )\n",
        "        self.flatten = nn.Flatten()     # convert to single array data\n",
        "    # specify how data passes through model, and how data are connected from input to output\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)           # flatten the tensor\n",
        "        logits = self.mlp(x)          # pass the tensor through the neural network\n",
        "        return logits\n",
        "\n",
        "# model initialization\n",
        "model = OurMLP().to(device)   # instance the model and move it to the device\n",
        "\n",
        "\"\"\" # maronn - ci proviamo\n",
        "X = torch.rand(1,28,28)     # creating a single 28x28 grayscale image\n",
        "predictions = model(X)\n",
        "probability = nn.Softmax(dim=1)(predictions)    # normalize to 0-1 probability\n",
        "y = probability.argmax(1)       # take most likely label\n",
        "\n",
        "print(f'predicted class: {y}') \"\"\"\n",
        "\n",
        "####### train the model yayyyyyy #######\n",
        "\n",
        "# define hyperparameters (the model cannot learn them) - the holy three - play with these\n",
        "epochs = 6                                             # how many times should our model analyze the dataset\n",
        "batch_size = 64                                        # number of samples that we get from the dataset each time (lower if crash)\n",
        "learning_rate = 0.0001                                 # amount of change allowed on weights in backpropagation (changeable in various epochs in certain techniques)\n",
        "\n",
        "# we have to define the training loop (automatically done in sklearn)\n",
        "# we aim for variability in each epoch, so choosing to increment epochs vs batch_size depends on the dataset\n",
        "\n",
        "# define the loss function - computes the differences between our prediction and the correct labels and is used to update weights on backpropagation\n",
        "loss_fn = nn.CrossEntropyLoss()     # uses negative log likelihood\n",
        "\n",
        "# define the optimizer - the mathematical approach used to compute the gradient\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)   # stochastic gradient descent, model.parameters() are the weights\n",
        "\n",
        "# train the model....? define the training loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):    #get samples from dataset, model, loss fn, optimizer\n",
        "    # get batch from dataset (X=data, y=label)\n",
        "    for batch, (X,y), in enumerate(dataloader):\n",
        "        # move data on GPU (tensor device inconsistency)\n",
        "        X_gpu = X.to(device)\n",
        "        size = len(dataloader)      # sample in datasets\n",
        "        # compute prediction and loss\n",
        "        pred = model(X_gpu)\n",
        "        y_tensor = torch.tensor([y])       # AS LIST SENNO SCOPPIA\n",
        "        y_tensor_gpu = y_tensor.to(device)\n",
        "        loss = loss_fn(pred, y_tensor_gpu)     # difference between true and predicted\n",
        "\n",
        "        # backward pass (backpropagation)\n",
        "        loss.backward()             # backpropagate error\n",
        "        optimizer.step()            # compute derivation\n",
        "        optimizer.zero_grad()       # clean gradient\n",
        "\n",
        "        # print loss during training (verbose)\n",
        "        if batch % 500 == 0:        # every 500 iterations\n",
        "            loss, current = loss.item(), (batch+1)*len(X)       # loss number\n",
        "            print(f'loss: {loss} [{current}/{size}]')\n",
        "\n",
        "# daje forte co sto training\n",
        "for t in range(epochs):\n",
        "    print(f'epoch: {t}')\n",
        "    train_loop(training_data, model, loss_fn, optimizer)\n",
        "\n",
        "print('daje lupetti')"
      ]
    }
  ]
}